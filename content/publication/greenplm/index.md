---
abstract: "Large pre-trained models have revolutionized natural language processing (NLP) research and applications, but high training costs and limited data resources have prevented their benefits from being shared equally amongst speakers of all the world's languages. To address issues of cross-linguistic access to such models and reduce energy consumption for sustainability during large-scale model training, this study proposes an effective and energy-efficient framework called GreenPLM that uses bilingual lexicons to directly ``translate'' pre-trained language models of one language into another at almost no additional cost. We validate this approach in 18 languages' BERT models and show that this framework is comparable to, if not better than, other heuristics with high training costs. In addition, given lightweight continued pre-training on limited data where available, this framework outperforms the original monolingual language models in six out of seven tested languages with up to 200x less pre-training. Aiming at the Leave No One Behind Principle (LNOB), our approach manages to reduce inequalities between languages and energy consumption greatly. We make our code and models publicly available."
slides: ""
url_pdf: "https://arxiv.org/abs/2211.06993"
publication_types:
  - "1"
authors:
  - Qingcheng Zeng
  - Lucas Garay
  - Peilin Zhou
  - Dading Chong
  - Yining Hua
  - Jiageng Wu
  - Yikang Pan
  - Han Zhou
  - Jie Yang
author_notes: []
publication: "*The 32nd International Joint Conference on Artificial Intelligence (IJCAI), 2023.*"
summary: "GreenPLM: Cross-Lingual Transfer of Monolingual Large Language Models at Almost No Cost"
url_dataset: ""
url_project: ""
publication_short: "The 32nd International Joint Conference on Artificial Intelligence (IJCAI)"
url_source: ""
url_video: ""
title: "GreenPLM: Cross-Lingual Transfer of Monolingual Large Language Models at Almost No Cost"
doi: ""
featured: true
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
date: 2023-05-26T00:00:00.000Z
url_slides: ""
publishDate: 2023-05-26T00:00:00.000Z
url_poster: ""
url_code: ""
---
