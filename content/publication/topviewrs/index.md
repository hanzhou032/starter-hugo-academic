---
abstract: "Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of `non-human' agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs remain unattested and underexplored. In this work, we thus study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks."
slides: ""
url_pdf: "https://arxiv.org/pdf/2406.02537v1"
publication_types:
  - "3"
authors:
  - Chengzu Li
  - Caiqi Zhang
  - admin
  - Nigel Collier
  - Anna Korhonen
  - Ivan Vulić
author_notes: []
publication: "The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
summary: "TopViewRS: Vision-Language Models as Top-View Spatial Reasoners."
url_dataset: ""
url_project: ""
publication_short: "The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
url_source: ""
url_video: ""
title: "TopViewRS: Vision-Language Models as Top-View Spatial Reasoners"
doi: ""
featured: false
tags: 
  - "Spatial Reasoning"
  - "Vision Language Models"
projects: []
image:
  caption: ""
  focal_point: “”
  preview_only: false
date: 2024-11-11T00:00:00.000Z
url_slides: ""
publishDate: 2024-11-11T00:00:00.000Z
url_poster: ""
url_code: "https://github.com/cambridgeltl/topviewrs"
links:
- name: "Abstract"
  url: https://arxiv.org/abs/2406.02537
- name: "Project Page"
  url: https://topviewrs.github.io/
- name: "ACL Anthology"
  url: https://aclanthology.org/2024.emnlp-main.106/
---
